{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b6ec86-c415-44d4-8d10-dfa5061c95d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def apply_cast(df,casts:dict):\n",
    "    # casts:{'col':\"long\"/\"string\"/\"date\"/\"ts\"}\n",
    "    for c,t in casts.items():\n",
    "        if t==\"date\":\n",
    "            df=df.withColumn(c,F.to_date(F.col(c)))\n",
    "        elif t==\"timestamp\":\n",
    "            df=df.withColumn(c,F.to_timestamp(F.col(c)))\n",
    "        else:\n",
    "            df=df.withColumn(c,F.col(c).cast(t))\n",
    "    return df\n",
    "\n",
    "def add_event_key(df,key_cols,existing_key_col=None,sep=\"||\"):\n",
    "    # if existing event key, use it, else create\n",
    "    if existing_key_col:\n",
    "        df=df.withColumn(\"event_key\",\n",
    "                         F.when(\n",
    "                             F.col(existing_key_col).isNotNull()&(F.col(existing_key_col)!=\"\"),\n",
    "                             F.col(existing_key_col).cast(\"string\")\n",
    "                         ).otherwise(F.sha2(F.concat_ws(sep,*[F.coalesce(F.col(c).cast(\"string\"),F.lit(\"\")) for c in key_cols]),\n",
    "                                            256\n",
    "                                            )\n",
    "                                     )\n",
    "                         )\n",
    "    else:\n",
    "        return df.withColumn(\"event_key\",\n",
    "                         F.sha2(F.concat_ws(sep,*[F.coalesce(F.col(c).cast(\"string\"),F.lit(\"\")) for c in key_cols]),\n",
    "                                256\n",
    "                         )\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def dedupe_lateste(df,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    w=Window.partitionBy(key_col).orderBy(F.col(order_col).desc_nulls_last())\n",
    "    return df.withColumn(\"rn\",F.row_number().over(w)).filter(\"rn=1\").drop(\"rn\")\n",
    "\n",
    "def ensure_table_schema(df,silver_tbl,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    if not spark.catalog.tableExists(silver_tbl):\n",
    "        w=Window.partitionBy(F.col(key_col)).orderBy(F.col(order_col).desc_nulls_last())\n",
    "        final_df=(df.withColumn(\"rn\",F.row_number().over(w)).filter(\"rn=1\").drop('rn'))\n",
    "        final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tbl)\n",
    "\n",
    "def get_max_dt(silver_tbl,order_col):\n",
    "    if not spark.catalog.tableExists(silver_tbl):\n",
    "        return None\n",
    "    return spark.sql(f\"SELECT max({order_col}) AS max_dt FROM {silver_tbl}\").collect()[0]['max_dt']\n",
    "\n",
    "def apply_lookback(df,max_dt,order_col,lookback_days=2):\n",
    "    if max_dt is None:\n",
    "        return df\n",
    "    return df.filter(F.col(order_col)>=F.date_sub(F.lit(max_dt),lookback_days))\n",
    "\n",
    "def merge_delta(silver_tbl,staging_df,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    staging_df.createOrReplaceTempView(\"stg_upsert\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {silver_tbl} t\n",
    "    USING stg_upsert s\n",
    "    ON s.{key_col}=t.{key_col}\n",
    "    WHEN MATCHED AND coalesce(s.{order_col},to_date('1900-01-01'))>coalesce(t.{order_col},to_date('1900-01-01'))\n",
    "    THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ddc173-fccb-46aa-9bb4-2e3d972919ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CFG = {\n",
    "\"bronze_tbl\": \"bronze_channel\", # <-- change to your real bronze table name\n",
    "\"silver_tbl\": \"silver_channel\",\n",
    "\"order_col\": \"ROW_INSERT_DATE\",\n",
    "\"lookback_days\": 2,\n",
    "\"casts\": {\n",
    "\"CLIENT_ID\": \"long\",\n",
    "\"BANKING_CHANNEL_NAME\": \"string\",\n",
    "\"INTERACTION_DATE\": \"date\",\n",
    "\"INTERACTION_TYPE\": \"string\",\n",
    "\"INTERACTION_ID\": \"string\",\n",
    "\"ROW_INSERT_DATE\": \"date\"\n",
    "},\n",
    "# Preferred if INTERACTION_ID is present & stable:\n",
    "\"key_cols\": [\"CLIENT_ID\", \"INTERACTION_ID\"]\n",
    "}\n",
    "\n",
    "# 1) Read bronze\n",
    "b = spark.table(CFG[\"bronze_tbl\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c5ae6b-6e30-47ed-856a-60576f37aee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    # 2) Clean/cast\n",
    "s = apply_cast(b, CFG[\"casts\"])\n",
    "s.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227dcae3-789c-48e8-8424-be3d09b8bda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3) Add deterministic event_key\n",
    "s = add_event_key(s, CFG[\"key_cols\"],existing_key_col='INTERACTION_ID',sep=\"||\")\n",
    "s.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c73b17-87fa-4efa-bd6e-fce06c42a92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4) Ensure silver table exists with correct schema (incl. event_key)\n",
    "ensure_table_schema(s, CFG[\"silver_tbl\"])\n",
    "spark.table('silver_channel').display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d93027-03b9-4fce-96ac-06007575a28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5) Incremental lookback window\n",
    "max_dt = get_max_dt(CFG[\"silver_tbl\"], CFG[\"order_col\"])\n",
    "inc = apply_lookback(s, max_dt, CFG[\"order_col\"], CFG[\"lookback_days\"])\n",
    "\n",
    "# 6) Deduplicate within this batch\n",
    "upsert_df = dedupe_lateste(inc, key_col=\"event_key\", order_col=CFG[\"order_col\"])\n",
    "upsert_df.display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d78177-04d4-4624-a4c4-4621d311874b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7) Idempotent upsert into silver (Delta MERGE)\n",
    "merge_delta(CFG[\"silver_tbl\"], upsert_df, key_col=\"event_key\", order_col=CFG[\"order_col\"])\n",
    "\n",
    "# 8) Quick validate\n",
    "spark.table(CFG[\"silver_tbl\"]).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_banking_channel_activities",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
