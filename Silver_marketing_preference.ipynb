{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89be4f80-c143-4415-a027-9ba9f9b93d93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def apply_cast(df,casts:dict):\n",
    "    # casts:{'col':\"long\"/\"string\"/\"date\"/\"ts\"}\n",
    "    for c,t in casts.items():\n",
    "        if t==\"date\":\n",
    "            df=df.withColumn(c,F.to_date(F.col(c)))\n",
    "        elif t==\"timestamp\":\n",
    "            df=df.withColumn(c,F.to_timestamp(F.col(c)))\n",
    "        else:\n",
    "            df=df.withColumn(c,F.col(c).cast(t))\n",
    "    return df\n",
    "\n",
    "def add_event_key(df,key_cols,existing_key_col=None,sep=\"||\"):\n",
    "    # if existing event key, use it, else create\n",
    "    if existing_key_col:\n",
    "        df=df.withColumn(\"event_key\",\n",
    "                         F.when(\n",
    "                             F.col(existing_key_col).isNotNull()&(F.col(existing_key_col)!=\"\"),\n",
    "                             F.col(existing_key_col).cast(\"string\")\n",
    "                         ).otherwise(F.sha2(F.concat_ws(sep,*[F.coalesce(F.col(c).cast(\"string\"),F.lit(\"\")) for c in key_cols]),\n",
    "                                            256\n",
    "                                            )\n",
    "                                     )\n",
    "                         )\n",
    "    else:\n",
    "        return df.withColumn(\"event_key\",\n",
    "                         F.sha2(F.concat_ws(sep,*[F.coalesce(F.col(c).cast(\"string\"),F.lit(\"\")) for c in key_cols]),\n",
    "                                256\n",
    "                         )\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def dedupe_lateste(df,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    w=Window.partitionBy(key_col).orderBy(F.col(order_col).desc_nulls_last())\n",
    "    return df.withColumn(\"rn\",F.row_number().over(w)).filter(\"rn=1\").drop(\"rn\")\n",
    "\n",
    "def ensure_table_schema(df,silver_tbl,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    if not spark.catalog.tableExists(silver_tbl):\n",
    "        w=Window.partitionBy(F.col(key_col)).orderBy(F.col(order_col).desc_nulls_last())\n",
    "        final_df=(df.withColumn(\"rn\",F.row_number().over(w)).filter(\"rn=1\").drop('rn'))\n",
    "        final_df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tbl)\n",
    "\n",
    "def get_max_dt(silver_tbl,order_col):\n",
    "    if not spark.catalog.tableExists(silver_tbl):\n",
    "        return None\n",
    "    return spark.sql(f\"SELECT max({order_col}) AS max_dt FROM {silver_tbl}\").collect()[0]['max_dt']\n",
    "\n",
    "def apply_lookback(df,max_dt,order_col,lookback_days=2):\n",
    "    if max_dt is None:\n",
    "        return df\n",
    "    return df.filter(F.col(order_col)>=F.date_sub(F.lit(max_dt),lookback_days))\n",
    "\n",
    "def merge_delta(silver_tbl,staging_df,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    staging_df.createOrReplaceTempView(\"stg_upsert\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {silver_tbl} t\n",
    "    USING stg_upsert s\n",
    "    ON s.{key_col}=t.{key_col}\n",
    "    WHEN MATCHED AND coalesce(s.{order_col},to_date('1900-01-01'))>coalesce(t.{order_col},to_date('1900-01-01'))\n",
    "    THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "def add_scd2_cols(df,start_col,track_cols,open_end=\"9999-12-31\"):\n",
    "    hash_exprs = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in track_cols]\n",
    "    return (df\n",
    "            .withColumn(\"row_hash\", F.sha2(F.concat_ws(\"||\", *hash_exprs), 256))\n",
    "            .withColumn(\"effective_start_date\", F.to_date(F.col(start_col)))\n",
    "            .withColumn(\"effective_end_date\", F.to_date(F.lit(open_end)))\n",
    "            .withColumn(\"is_current\", F.lit(True))\n",
    "    )\n",
    "\n",
    "def merge_data_SCD2(\n",
    "    silver_tbl: str,\n",
    "    staging_df,\n",
    "    natural_keys=list,\n",
    "    track_cols= list, # preference columns to track (exclude ROW_* audit cols)\n",
    "    start_col: str = \"CONSENT_UPDATE_TIMESTAMP\",\n",
    "    open_end: str = \"9999-12-31\"\n",
    "):\n",
    "    # 1) prepare SCD2 staging (row_hash + effective dates + current flag)\n",
    "    hash_exprs = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in track_cols]\n",
    "    u = staging_df.withColumn(\"_op\", F.lit(\"U\"))\n",
    "    i = staging_df.withColumn(\"_op\", F.lit(\"I\"))\n",
    "\n",
    "    stg = u.unionByName(i)\n",
    "    stg.createOrReplaceTempView(\"stg_scd2\")\n",
    "\n",
    "    on_keys = \" AND \".join([f\"t.{k}=s.{k}\" for k in natural_keys])\n",
    "\n",
    "    # 2) Close current rows when changed (hash differs)\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {silver_tbl} AS t\n",
    "        USING stg_scd2 AS s\n",
    "        ON ({on_keys} AND t.is_current = true)\n",
    "        WHEN MATCHED AND s._op='U' AND t.row_hash <> s.row_hash THEN\n",
    "          UPDATE SET\n",
    "            t.effective_end_date = date_sub(s.effective_start_date, 1),\n",
    "            t.is_current = false\n",
    "        WHEN NOT MATCHED AND s._op='I' THEN\n",
    "          INSERT *\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51afbb8c-6f02-4e9a-84eb-689b0a11e800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "\"bronze_tbl\": \"bronze_consent\", \n",
    "\"silver_tbl\": \"silver_consent\", \n",
    "\"natural_keys\": [\"CLIENT_ID\",\"CONSENT_SOURCE\"], # SCD2 natural key\n",
    "\"start_col\": \"CONSENT_UPDATE_TIMESTAMP\", # prefer consent update time;\n",
    "\"order_col\": \"ROW_INSERT_DATE\", # incremental watermark column (ingestion time)\n",
    "\"lookback_days\": 2,\n",
    "\n",
    "# Cast types (add/remove columns to match your file)\n",
    "\"casts\": {\n",
    "\"CLIENT_ID\": \"long\",\n",
    "\"CONSENT_UPDATE_FIELD\": \"string\",\n",
    "\"CONSENT_UPDATE_TIMESTAMP\": \"timestamp\",\n",
    "\"CONSENT_SOURCE\": \"string\",\n",
    "\"ROW_INSERT_DATE\": \"date\"\n",
    "},\n",
    "\"key_cols\": [\"CLIENT_ID\", \"CONSENT_UPDATE_FIELD\", \"CONSENT_UPDATE_TIMESTAMP\"],\n",
    "\n",
    "\n",
    "# Columns that define \"business change\" (DO NOT include ROW_* audit cols)\n",
    "\"track_cols\": [\n",
    "\"CLIENT_ID\",\n",
    "\"CONSENT_UPDATE_TIMESTAMP\",\n",
    "\"CONSENT_SOURCE\"\n",
    "]\n",
    "# add your opt-in flags here if you have them, e.g. \"EMAIL_OPT_IN\",\"SMS_OPT_IN\",\"PUSH_OPT_IN\"\n",
    "\n",
    "}\n",
    "\n",
    "# 1) Read bronze\n",
    "b = spark.table(CFG[\"bronze_tbl\"])\n",
    "b.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d90a0d8-9e3b-4944-b5fb-a54b489788f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2) Cast / clean\n",
    "s = apply_cast(b, CFG[\"casts\"])\n",
    "s.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5295bbcc-9227-4203-bf86-8b6e1f96c1b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) Add deterministic event_key\n",
    "s = add_event_key(s, CFG[\"key_cols\"],existing_key_col=None,sep=\"||\")\n",
    "s.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a66b546-b2e0-400d-8d76-9a88635ab042",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770318586961}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s_scd2=add_scd2_cols(s,start_col=\"CONSENT_UPDATE_TIMESTAMP\",track_cols=CFG[\"track_cols\"])\n",
    "s_scd2.display()\n",
    "# 4) Ensure silver table exists with correct schema (incl. event_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e242ae8f-22d6-4a65-b3f8-1e31343a40d5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770316734011}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4) Ensure silver table exists with correct schema (incl. event_key)\n",
    "ensure_table_schema(s_scd2, CFG[\"silver_tbl\"])\n",
    "spark.table('silver_consent').display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d865baa-dab8-4b37-9753-e2d9659dcbed",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"event_key\":250,\"row_hash\":522},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770321568853}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5) Incremental lookback window\n",
    "max_dt = get_max_dt(CFG[\"silver_tbl\"], CFG[\"order_col\"])\n",
    "inc = apply_lookback(s_scd2, max_dt, CFG[\"order_col\"], CFG[\"lookback_days\"])\n",
    "\n",
    "# 6) Deduplicate within this batch\n",
    "upsert_df = dedupe_lateste(inc, key_col=\"event_key\", order_col=CFG[\"order_col\"])\n",
    "upsert_df.display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52dc8160-3d09-4469-8ebc-7d7bbe6fddca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#s_inc=dedupe_lateste(upsert_df,key_col=CFG['natural_keys'],order_col=\"CONSENT_UPDATE_TIMESTAMP\")\n",
    "#s_inc.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b56f353-7459-434b-9d18-bac83736e2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6) One-merge SCD2 upsert\n",
    "merge_data_SCD2(\n",
    "silver_tbl=CFG[\"silver_tbl\"],\n",
    "staging_df=upsert_df,\n",
    "natural_keys=CFG[\"natural_keys\"],\n",
    "track_cols=CFG[\"track_cols\"],\n",
    "start_col=CFG[\"start_col\"]\n",
    ")\n",
    "\n",
    "# 7) Quick check\n",
    "display(\n",
    "spark.table(CFG[\"silver_tbl\"])\n",
    ".orderBy(F.col(\"CLIENT_ID\"), F.col(\"effective_start_date\"))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_marketing_preference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
