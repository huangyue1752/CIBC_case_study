{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d83da08-5a8d-48af-b50e-9ac1d8482235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def apply_cast(df,casts:dict):\n",
    "    # casts:{'col':\"long\"/\"string\"/\"date\"/\"ts\"}\n",
    "    for c,t in casts.items():\n",
    "        if t==\"date\":\n",
    "            df=df.withColumn(c,F.to_date(F.col(c)))\n",
    "        elif t==\"timestamp\":\n",
    "            df=df.withColumn(c,F.to_timestamp(F.col(c)))\n",
    "        else:\n",
    "            df=df.withColumn(c,F.col(c).cast(t))\n",
    "    return df\n",
    "\n",
    "def add_event_key(df,key_cols,existing_key_col=None,sep=\"||\"):\n",
    "    # if existing event key, use it, else create\n",
    "    if existing_key_col:\n",
    "        df=df.withColumn(\"event_key\",\n",
    "                         F.when(\n",
    "                             F.col(existing_key_col).isNotNull()&(F.col(existing_key_col)!=\"\"),\n",
    "                             F.col(existing_key_col).cast(\"string\")\n",
    "                         ).otherwise(F.sha2(F.concat_ws(sep,*[F.coelesce(F.col(c).cast(\"string\"),F.lit(\"\")) for c in key_cols]),\n",
    "                                            256\n",
    "                                            )\n",
    "                                     )\n",
    "                         )\n",
    "    else:\n",
    "        return df.withColumn(\"event_key\",\n",
    "                         F.sha2(F.concat_ws(sep,*[F.coelesce(F.col(c).cast(\"string\"),F.lit(\"\")) for c in key_cols]),\n",
    "                                256\n",
    "                         )\n",
    "        )\n",
    "\n",
    "def dedupe_lateste(df,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    w=Window.partitionBy(key_col).orderBy(F.col(order_col).desc_nulls_last())\n",
    "    return df.withColumn(\"rn\",F.row_number().over(w)).filter(\"rn=1\").drop(\"rn\")\n",
    "\n",
    "def ensure_table_schema(df,silver_tbl):\n",
    "    if not spark.catalog.tableExists(silver_tbl):\n",
    "        df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tbl)\n",
    "\n",
    "def get_max_dt(silver_tbl,order_col):\n",
    "    if not spark.catalog.tableExists(silver_tbl):\n",
    "        return None\n",
    "    return spark.sql(f\"SELECT max({order_col}) AS max_dt FROM {silver_tbl}\").collect()[0]['max_dt']\n",
    "\n",
    "def apply_lookback(df,max_dt,order_col,lookback_days=2):\n",
    "    if max_dt is None:\n",
    "        return df\n",
    "    return df.filter(F.col(order_col)>=F.date_sub(F.lit(max_dt),lookback_days))\n",
    "\n",
    "def merge_delta(silver_tbl,staging_df,key_col=\"event_key\",order_col=\"ROW_INSERT_DATE\"):\n",
    "    staging_df.createOrReplaceTempView(\"stg_upsert\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {silver_tbl} t\n",
    "    USING stg _upsert s\n",
    "    ON s.{key_col}=t.{key_col}\n",
    "    WHEN MATCHED AND coalesce(s.{order_col},to_date('1900-01-01'))>coalesce(t.{order_col},to_date('1900-01-01'))\n",
    "    THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
